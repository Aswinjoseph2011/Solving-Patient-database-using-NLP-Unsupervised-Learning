{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efcc101a",
   "metadata": {},
   "source": [
    "###  Solving Patient database using NLP Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454e4839",
   "metadata": {},
   "source": [
    "#### We are going to use LDA ( *Latent Dirichlet Allocation* ) - Classify text in a document to a particular topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13d1826",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e16ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"Patient_Details.csv\")\n",
    "data[\"index\"] = data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e351b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n"
     ]
    }
   ],
   "source": [
    "document = data\n",
    "print(len(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32e47fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Right side of epiglottis swelled up and hinder...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Approximately 30 min post vaccination administ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>About 15 minutes after receiving the vaccine, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>extreme fatigue, dizziness,. could not lift my...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Injection site swelling, redness, warm to the ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TEXT  index\n",
       "0  Right side of epiglottis swelled up and hinder...      0\n",
       "1  Approximately 30 min post vaccination administ...      1\n",
       "2  About 15 minutes after receiving the vaccine, ...      2\n",
       "3  extreme fatigue, dizziness,. could not lift my...      3\n",
       "4  Injection site swelling, redness, warm to the ...      4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21218556",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "1.  Tokenization, sentences to word , Lower_case, remove_punchutation\n",
    "2.  Words that are fewer that character are removed\n",
    "3.  All stopwords are removed\n",
    "4.  Words are lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e823bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0e2063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "## Lemmatizer Example:\n",
    "print(WordNetLemmatizer().lemmatize(\"went\",pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b7d6631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orginal_data</th>\n",
       "      <th>Stemmed_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cares</td>\n",
       "      <td>care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>files</td>\n",
       "      <td>file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>defined</td>\n",
       "      <td>defin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  orginal_data Stemmed_word\n",
       "0        cares         care\n",
       "1        files         file\n",
       "2         dies          die\n",
       "3      defined        defin\n",
       "4         died          die"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Stemmer Example:\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "original_word = [\"cares\",\"files\",\"dies\",\"defined\",\"died\"]\n",
    "singles=[stemmer.stem(plural) for plural in original_word]\n",
    "pd.DataFrame(data={\"orginal_data\":original_word,\"Stemmed_word\":singles})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746c87fe",
   "metadata": {},
   "source": [
    "### Creating a Function to do Pre-Processing step on Entire Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d288587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text,pos=\"v\"))\n",
    "\n",
    "# Tokenize and Lemmatize\n",
    "\n",
    "def preprocessing(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            #apply lemmatizing_stemming() on the token, ten add to that result list\n",
    "            result.append(lemmatizing_stemming(token))\n",
    "    return result\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eefd73",
   "metadata": {},
   "source": [
    "### Preview the document after Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b0822e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['Moderna', 'COVID?19', 'Vaccine', 'EUA', '', 'Arm', 'severe', 'pain,', 'redness,', 'hard', 'hives', 'that', 'keep', 'worsening.', '', 'Fever', 'Chills', 'Body', 'aches', 'Headache', 'Nausea', 'and', 'vomiting', 'Tachycardia', 'with', 'hypertension', 'Dizzy', 'Sweats', '', 'I', 'felt', 'worst', 'after', 'vaxx', 'than', 'I', 'ever', 'did', 'with', 'COVID.']\n",
      "\n",
      " \n",
      " Tokenized and Lemmatized document : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['moderna',\n",
       " 'covid',\n",
       " 'vaccin',\n",
       " 'sever',\n",
       " 'pain',\n",
       " 'red',\n",
       " 'hard',\n",
       " 'hive',\n",
       " 'worsen',\n",
       " 'fever',\n",
       " 'chill',\n",
       " 'bodi',\n",
       " 'ach',\n",
       " 'headach',\n",
       " 'nausea',\n",
       " 'vomit',\n",
       " 'tachycardia',\n",
       " 'hypertens',\n",
       " 'dizzi',\n",
       " 'sweat',\n",
       " 'felt',\n",
       " 'worst',\n",
       " 'vaxx',\n",
       " 'covid']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_number = 100\n",
    "doc_sample = document[document[\"index\"]== document_number].values[0][0]\n",
    "print(\"Original document: \")\n",
    "words=[]\n",
    "for i in doc_sample.split(\" \"):\n",
    "    words.append(i)\n",
    "print(words)\n",
    "print(\"\\n \\n Tokenized and Lemmatized document : \")\n",
    "preprocessing(doc_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "998f46e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Right side of epiglottis swelled up and hinder...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Approximately 30 min post vaccination administ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>About 15 minutes after receiving the vaccine, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>extreme fatigue, dizziness,. could not lift my...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Injection site swelling, redness, warm to the ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>Tachycardia with a heart rate in the 120-140s,...</td>\n",
       "      <td>494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Strong chills, with uncontrollable and vigorou...</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Fever chills severe myalgia headache</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Middle of the night woke up shivering, chills,...</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Diarrhea for 3 hours</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  TEXT  index\n",
       "0    Right side of epiglottis swelled up and hinder...      0\n",
       "1    Approximately 30 min post vaccination administ...      1\n",
       "2    About 15 minutes after receiving the vaccine, ...      2\n",
       "3    extreme fatigue, dizziness,. could not lift my...      3\n",
       "4    Injection site swelling, redness, warm to the ...      4\n",
       "..                                                 ...    ...\n",
       "494  Tachycardia with a heart rate in the 120-140s,...    494\n",
       "495  Strong chills, with uncontrollable and vigorou...    495\n",
       "496               Fever chills severe myalgia headache    496\n",
       "497  Middle of the night woke up shivering, chills,...    497\n",
       "498                               Diarrhea for 3 hours    498\n",
       "\n",
       "[499 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388bb2fc",
   "metadata": {},
   "source": [
    "###  Now Preprocess all the Text we have, to do that lets use map function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d252f1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [right, epiglotti, swell, hinder, swallow, pic...\n",
       "1    [approxim, post, vaccin, administr, patient, d...\n",
       "2    [minut, receiv, vaccin, patient, complain, lea...\n",
       "3            [extrem, fatigu, dizzi, lift, leav, hour]\n",
       "4       [inject, site, swell, red, warm, touch, itchi]\n",
       "5      [patient, call, state, throat, swell, benadryl]\n",
       "6    [sever, chill, approxim, hour, receiv, vaccin,...\n",
       "7                           [nasal, congest, diarrhea]\n",
       "8    [follow, vaccin, notic, rais, itchi, patch, va...\n",
       "9             [hive, rash, bodi, go, away, day, begin]\n",
       "Name: TEXT, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_doc = document[\"TEXT\"]. map(preprocessing)\n",
    "preprocess_doc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336552e9",
   "metadata": {},
   "source": [
    "### Bag of words on the dataset\n",
    "\n",
    "#### Create a dictionary from \"preprocess_doc\" containing the number of times a word appears in the training set. To do that lets pass preprocess_docs to **gensim.corpora.Dictionary()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "44a57de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(preprocess_doc)  ## Special purpose Dictionary for NLP task\n",
    "tokenized_reviews = preprocess_doc\n",
    "doc_term_matrix = [dictionary.doc2bow(rev) for rev in tokenized_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c1ef9e",
   "metadata": {},
   "source": [
    "#### Checking dictionary created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96cab378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 benadryl\n",
      "1 epiglotti\n",
      "2 hinder\n",
      "3 pictur\n",
      "4 right\n",
      "5 swallow\n",
      "6 swell\n",
      "7 take\n",
      "8 tylenol\n",
      "9 administ\n",
      "10 administr\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k,v in dictionary.iteritems():\n",
    "    print(k,v)\n",
    "    count+=1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51606a90",
   "metadata": {},
   "source": [
    "### Fliter out the words that are extreme and Not comman\n",
    "1. no_below = 5\n",
    "2. no_above = 0.5\n",
    "3. keep_n = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46ab9095",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=5,no_above=0.5,keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d209c0",
   "metadata": {},
   "source": [
    "### Converts list of words into bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3dc9b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25, 1),\n",
       " (43, 1),\n",
       " (49, 1),\n",
       " (54, 1),\n",
       " (56, 1),\n",
       " (59, 1),\n",
       " (61, 1),\n",
       " (74, 1),\n",
       " (77, 1),\n",
       " (79, 1),\n",
       " (82, 1),\n",
       " (84, 1),\n",
       " (91, 1),\n",
       " (119, 2),\n",
       " (144, 1),\n",
       " (151, 1),\n",
       " (165, 1),\n",
       " (177, 1),\n",
       " (183, 1),\n",
       " (233, 1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in preprocess_doc]\n",
    "bow_corpus[document_number]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baad782",
   "metadata": {},
   "source": [
    "### Preview Bag of Words for our Sample preprocessed document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de9045cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word25 (\"vaccin\") appears 1 times\n",
      "word43 (\"dizzi\") appears 1 times\n",
      "word49 (\"red\") appears 1 times\n",
      "word54 (\"chill\") appears 1 times\n",
      "word56 (\"fever\") appears 1 times\n",
      "word59 (\"pain\") appears 1 times\n",
      "word61 (\"sever\") appears 1 times\n",
      "word74 (\"bodi\") appears 1 times\n",
      "word77 (\"hive\") appears 1 times\n",
      "word79 (\"ach\") appears 1 times\n",
      "word82 (\"felt\") appears 1 times\n",
      "word84 (\"headach\") appears 1 times\n",
      "word91 (\"nausea\") appears 1 times\n",
      "word119 (\"covid\") appears 2 times\n",
      "word144 (\"tachycardia\") appears 1 times\n",
      "word151 (\"hard\") appears 1 times\n",
      "word165 (\"worsen\") appears 1 times\n",
      "word177 (\"vomit\") appears 1 times\n",
      "word183 (\"sweat\") appears 1 times\n",
      "word233 (\"moderna\") appears 1 times\n"
     ]
    }
   ],
   "source": [
    "bow_doc_100 = bow_corpus[document_number]\n",
    "for i in range(len(bow_doc_100)):\n",
    "    print(\"word{} (\\\"{}\\\") appears {} times\".format(bow_doc_100[i][0],dictionary[bow_doc_100[i][0]],bow_doc_100[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1f57d2",
   "metadata": {},
   "source": [
    "###  TF - IDF ( Term Frequency, Inverse Document Frequency) on the dataset. Giving weights to word on how many times it occurs in document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ab335b",
   "metadata": {},
   "source": [
    "### Create tf-idf model using model.TfidfModel on \"bow_corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "de49a4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=499, num_nnz=6619)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora,models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06b2a2f",
   "metadata": {},
   "source": [
    "### Apply transformation to entire Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0ef4b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 0.17430971975028317), (7, 0.18076565842755962), (8, 0.29002599050411054), (9, 0.1685726803737903), (10, 0.22802810834630363), (11, 0.12765769340388383), (12, 0.20711414108771944), (13, 0.10130921074372404), (14, 0.1485716606624681), (15, 0.06898379841108326), (16, 0.22802810834630363), (17, 0.34272295582443013), (18, 0.4874242306538179), (19, 0.28336817215674964), (20, 0.15239416393643548), (21, 0.18432432383797243), (22, 0.17430971975028317), (23, 0.1385570565747788), (24, 0.17430971975028317), (25, 0.056460309399546084), (26, 0.22007698701347678)]\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf= tfidf[bow_corpus]\n",
    "print(corpus_tfidf[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199164f1",
   "metadata": {},
   "source": [
    "### Preview TF-IDF score for the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e835dd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.320698703854761),\n",
      " (1, 0.33015986313200013),\n",
      " (2, 0.5581792126555412),\n",
      " (3, 0.2142732968037797),\n",
      " (4, 0.5542319254783102),\n",
      " (5, 0.35142097225296565)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb3901e",
   "metadata": {},
   "source": [
    "### Run LDA using Bag of Words\n",
    "\n",
    "# Create a model\n",
    "### Hyper_Parameters\n",
    "1. num_topics\n",
    "2. id2words\n",
    "3. workers\n",
    "4. passes\n",
    "5. alpha and eta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cc8607",
   "metadata": {},
   "source": [
    "#### Train your LDA model using gensim.model.LdaMulticore and save it to \"lda_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e9575ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus,\n",
    "                                      num_topics= 10, \n",
    "                                      id2word = dictionary,\n",
    "                                      passes=2,\n",
    "                                      workers=2\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca69b9e8",
   "metadata": {},
   "source": [
    "### For each topic explore the words occuring in that topic and its relative weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a43cd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      " words: 0.064*\"patient\" + 0.039*\"vaccin\" + 0.034*\"leav\" + 0.024*\"covid\" + 0.018*\"report\" + 0.016*\"note\" + 0.015*\"swell\" + 0.014*\"minut\" + 0.013*\"headach\" + 0.013*\"pain\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      " words: 0.056*\"pain\" + 0.037*\"inject\" + 0.034*\"site\" + 0.032*\"headach\" + 0.031*\"sore\" + 0.025*\"vaccin\" + 0.024*\"chill\" + 0.022*\"fatigu\" + 0.022*\"muscl\" + 0.022*\"leav\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      " words: 0.057*\"headach\" + 0.037*\"hour\" + 0.036*\"chill\" + 0.035*\"fever\" + 0.032*\"fatigu\" + 0.032*\"ach\" + 0.028*\"nausea\" + 0.026*\"day\" + 0.025*\"bodi\" + 0.022*\"vaccin\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      " words: 0.048*\"inject\" + 0.035*\"site\" + 0.021*\"start\" + 0.020*\"benadryl\" + 0.020*\"sore\" + 0.020*\"itch\" + 0.019*\"day\" + 0.018*\"feel\" + 0.018*\"take\" + 0.018*\"hour\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      " words: 0.028*\"patient\" + 0.026*\"headach\" + 0.023*\"chill\" + 0.023*\"leav\" + 0.023*\"feel\" + 0.022*\"vaccin\" + 0.021*\"nausea\" + 0.019*\"felt\" + 0.019*\"pain\" + 0.016*\"swell\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      " words: 0.053*\"vaccin\" + 0.032*\"hour\" + 0.029*\"symptom\" + 0.023*\"chill\" + 0.021*\"receiv\" + 0.018*\"post\" + 0.016*\"site\" + 0.016*\"inject\" + 0.016*\"approxim\" + 0.015*\"fever\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      " words: 0.037*\"rash\" + 0.025*\"itchi\" + 0.022*\"red\" + 0.021*\"increas\" + 0.021*\"ach\" + 0.021*\"start\" + 0.021*\"bodi\" + 0.019*\"dizzi\" + 0.019*\"hive\" + 0.018*\"benadryl\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      " words: 0.055*\"pain\" + 0.043*\"inject\" + 0.035*\"site\" + 0.034*\"hour\" + 0.027*\"fever\" + 0.025*\"vaccin\" + 0.022*\"swell\" + 0.021*\"headach\" + 0.015*\"start\" + 0.014*\"take\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      " words: 0.044*\"inject\" + 0.039*\"hour\" + 0.034*\"sore\" + 0.030*\"area\" + 0.026*\"start\" + 0.026*\"vaccin\" + 0.024*\"chill\" + 0.021*\"site\" + 0.018*\"day\" + 0.017*\"headach\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      " words: 0.028*\"symptom\" + 0.027*\"felt\" + 0.026*\"feel\" + 0.025*\"take\" + 0.025*\"leav\" + 0.023*\"vaccin\" + 0.020*\"numb\" + 0.019*\"receiv\" + 0.018*\"go\" + 0.018*\"hour\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx,topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\n words: {}\".format(idx,topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe7b160",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e54e551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      " words: 0.023*\"inject\" + 0.021*\"sore\" + 0.019*\"leav\" + 0.019*\"site\" + 0.017*\"swell\" + 0.016*\"itch\" + 0.016*\"pain\" + 0.016*\"day\" + 0.014*\"headach\" + 0.013*\"muscl\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      " words: 0.031*\"dizzi\" + 0.028*\"fever\" + 0.024*\"chill\" + 0.023*\"myalgia\" + 0.020*\"nausea\" + 0.020*\"headach\" + 0.018*\"vomit\" + 0.016*\"hive\" + 0.016*\"sever\" + 0.011*\"ach\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      " words: 0.018*\"leav\" + 0.016*\"hour\" + 0.015*\"fever\" + 0.014*\"felt\" + 0.013*\"ach\" + 0.013*\"feel\" + 0.011*\"symptom\" + 0.011*\"vaccin\" + 0.011*\"heavi\" + 0.011*\"resolv\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      " words: 0.022*\"patient\" + 0.013*\"temp\" + 0.013*\"vomit\" + 0.013*\"report\" + 0.012*\"nausea\" + 0.012*\"advis\" + 0.012*\"give\" + 0.012*\"take\" + 0.011*\"start\" + 0.011*\"loss\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      " words: 0.017*\"slight\" + 0.017*\"pain\" + 0.017*\"headach\" + 0.013*\"lip\" + 0.013*\"area\" + 0.013*\"symptom\" + 0.012*\"fatigu\" + 0.012*\"shoot\" + 0.012*\"swell\" + 0.011*\"hour\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      " words: 0.021*\"vaccin\" + 0.018*\"chest\" + 0.016*\"patient\" + 0.013*\"covid\" + 0.012*\"increas\" + 0.011*\"sore\" + 0.011*\"administr\" + 0.011*\"hour\" + 0.011*\"swell\" + 0.011*\"test\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      " words: 0.026*\"fatigu\" + 0.024*\"chill\" + 0.022*\"headach\" + 0.021*\"inject\" + 0.021*\"hour\" + 0.021*\"pain\" + 0.020*\"fever\" + 0.019*\"ach\" + 0.019*\"bodi\" + 0.019*\"sore\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      " words: 0.025*\"inject\" + 0.023*\"site\" + 0.021*\"nausea\" + 0.018*\"pain\" + 0.016*\"rash\" + 0.016*\"red\" + 0.015*\"itchi\" + 0.014*\"right\" + 0.014*\"leav\" + 0.013*\"swell\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      " words: 0.020*\"pain\" + 0.016*\"day\" + 0.016*\"joint\" + 0.015*\"sore\" + 0.014*\"inject\" + 0.013*\"fatigu\" + 0.013*\"muscl\" + 0.013*\"site\" + 0.012*\"swell\" + 0.012*\"red\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      " words: 0.034*\"chill\" + 0.033*\"ach\" + 0.030*\"fever\" + 0.020*\"bodi\" + 0.019*\"grade\" + 0.018*\"headach\" + 0.013*\"start\" + 0.013*\"swell\" + 0.013*\"hour\" + 0.012*\"muscl\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics = 10, id2word = dictionary, passes = 2, workers = 4)\n",
    "for idx,topic in lda_model_tfidf.print_topics(-1):\n",
    "    print(\"Topic: {} \\n words: {}\".format(idx,topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3102ff1",
   "metadata": {},
   "source": [
    "###  Perform Evalution by Classifying the sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0b0b6ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moderna',\n",
       " 'covid',\n",
       " 'vaccin',\n",
       " 'sever',\n",
       " 'pain',\n",
       " 'red',\n",
       " 'hard',\n",
       " 'hive',\n",
       " 'worsen',\n",
       " 'fever',\n",
       " 'chill',\n",
       " 'bodi',\n",
       " 'ach',\n",
       " 'headach',\n",
       " 'nausea',\n",
       " 'vomit',\n",
       " 'tachycardia',\n",
       " 'hypertens',\n",
       " 'dizzi',\n",
       " 'sweat',\n",
       " 'felt',\n",
       " 'worst',\n",
       " 'vaxx',\n",
       " 'covid']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_doc[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4538b928",
   "metadata": {},
   "source": [
    "### Check which topic our test document belongs to using LDA Bag of Word Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cdcccd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Score : 0.6654343605041504 \t \n",
      " Topic: 0.073*\"ach\" + 0.057*\"bodi\" + 0.045*\"nausea\" + 0.043*\"headach\" + 0.038*\"chill\" + 0.032*\"fever\" + 0.030*\"sore\" + 0.024*\"fatigu\" + 0.019*\"start\" + 0.018*\"pain\"\n",
      "\n",
      " Score : 0.2981916069984436 \t \n",
      " Topic: 0.037*\"vaccin\" + 0.035*\"inject\" + 0.030*\"pain\" + 0.025*\"headach\" + 0.024*\"felt\" + 0.024*\"site\" + 0.023*\"hour\" + 0.022*\"symptom\" + 0.019*\"go\" + 0.019*\"swell\"\n"
     ]
    }
   ],
   "source": [
    "for index,score in sorted(lda_model[bow_corpus[document_number]],key=lambda tup:-1 * tup[1]):\n",
    "                            print(\"\\n Score : {} \\t \\n Topic: {}\".format(score,lda_model.print_topic(index,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e79c86",
   "metadata": {},
   "source": [
    "### Check using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "88fd802a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Score : 0.7769221663475037 \t \n",
      " Topic: 0.026*\"fatigu\" + 0.024*\"chill\" + 0.022*\"headach\" + 0.021*\"inject\" + 0.021*\"hour\" + 0.021*\"pain\" + 0.020*\"fever\" + 0.019*\"ach\" + 0.019*\"bodi\" + 0.019*\"sore\"\n",
      "\n",
      " Score : 0.18670441210269928 \t \n",
      " Topic: 0.031*\"dizzi\" + 0.028*\"fever\" + 0.024*\"chill\" + 0.023*\"myalgia\" + 0.020*\"nausea\" + 0.020*\"headach\" + 0.018*\"vomit\" + 0.016*\"hive\" + 0.016*\"sever\" + 0.011*\"ach\"\n"
     ]
    }
   ],
   "source": [
    "for index,score in sorted(lda_model_tfidf[bow_corpus[document_number]],key=lambda tup:-1 * tup[1]):\n",
    "                            print(\"\\n Score : {} \\t \\n Topic: {}\".format(score,lda_model_tfidf.print_topic(index,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d78e9a",
   "metadata": {},
   "source": [
    "### Testing the Model on Unseen Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7056a320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score : 0.5499391555786133 \t Topic : 0.047*\"hour\" + 0.037*\"vaccin\" + 0.035*\"headach\" + 0.031*\"leav\" + 0.023*\"swell\"\n",
      "score : 0.050014909356832504 \t Topic : 0.043*\"sore\" + 0.027*\"hour\" + 0.023*\"take\" + 0.020*\"felt\" + 0.020*\"vaccin\"\n",
      "score : 0.050013381987810135 \t Topic : 0.073*\"ach\" + 0.057*\"bodi\" + 0.045*\"nausea\" + 0.043*\"headach\" + 0.038*\"chill\"\n",
      "score : 0.05000939965248108 \t Topic : 0.047*\"pain\" + 0.034*\"vaccin\" + 0.030*\"leav\" + 0.026*\"headach\" + 0.021*\"feel\"\n",
      "score : 0.05000882223248482 \t Topic : 0.037*\"vaccin\" + 0.035*\"inject\" + 0.030*\"pain\" + 0.025*\"headach\" + 0.024*\"felt\"\n",
      "score : 0.05000732094049454 \t Topic : 0.062*\"chill\" + 0.042*\"fever\" + 0.041*\"ach\" + 0.037*\"swell\" + 0.036*\"hour\"\n",
      "score : 0.050003424286842346 \t Topic : 0.056*\"inject\" + 0.051*\"hour\" + 0.035*\"site\" + 0.024*\"vaccin\" + 0.023*\"fever\"\n",
      "score : 0.05000119283795357 \t Topic : 0.058*\"pain\" + 0.045*\"right\" + 0.039*\"chill\" + 0.029*\"sever\" + 0.029*\"headach\"\n",
      "score : 0.05000118911266327 \t Topic : 0.064*\"site\" + 0.053*\"inject\" + 0.042*\"day\" + 0.041*\"swell\" + 0.030*\"vaccin\"\n",
      "score : 0.05000118911266327 \t Topic : 0.080*\"patient\" + 0.022*\"vaccin\" + 0.021*\"symptom\" + 0.020*\"inject\" + 0.020*\"benadryl\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document= \"I have cold\"\n",
    "bow_vector = dictionary.doc2bow(preprocessing(unseen_document))\n",
    "for index,score in sorted(lda_model[bow_vector],key = lambda tup:-1*tup[1]):\n",
    "                          print(\"score : {} \\t Topic : {}\".format(score,lda_model.print_topic (index,5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
